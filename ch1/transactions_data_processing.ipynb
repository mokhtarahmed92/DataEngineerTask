{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge(1):\n",
    "----------------------------------\n",
    "## Subscriber data analysis using pyspark\n",
    "\n",
    "* The target of this task is to analyze subscriber data from telecommunication source system named DPI.\n",
    "\n",
    "## Input:\n",
    "-------------------------------------\n",
    "* Highly compressed file as tar + gzip and the extension .tgz The data schema provided as below  \n",
    "\n",
    "<img src=\"img/ch1_1.jpg\">\n",
    "\n",
    "## Ouput: \n",
    "------------------------------------\n",
    "1. New **CSV** file with the following columns :\n",
    "    * Account_Number : which map to the subscriber id in the input file \n",
    "    * MostUsedServiceName_1 : the 1st most used service by the subscriber \n",
    "    * MostUsedServiceName_2 : the 2nd most used service by the subscriber \n",
    "    * Event_Type : Hardcoded to 'Mobile'\n",
    "    * IMEI : Map to IMEI in the input file\n",
    "    * MAC_Address : Hardcoded to N/A\n",
    "    * Access_Point : Hardcoded to N/A\n",
    "    * Total_Bytes : Sum(TotalBytes) for the subscriberID \n",
    "    * Total_Count : Count of transactions\n",
    "    * Total_Time : Difference between (min(StartTime), max(EndTime)) for the subscriberID   \n",
    "    * Insertion_Date : Current_Date (yyyy-mm-dd) \n",
    "\n",
    "2. **dpiDuplicate** file which contains the duplicate transaction in the input file \n",
    "\n",
    "3. **dpiRejection** file which contains the rows of a bad subscriber_id **Which i assumed it will be an number**\n",
    "\n",
    "## Task Description:\n",
    "--------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* load the required spark apis and utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import * \n",
    "import pyspark.sql.functions as func\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define utilities and helper functions \n",
    "    * see(s): function to print a value \n",
    "    * see(s,v): function to pring a key and value \n",
    "    * get_most_common(arr, ith): function to return the ith commen used item in a list \n",
    "    * is_int(num): check if a value is an integer or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def see(s):\n",
    "    \"\"\"\n",
    "    Function to print a value to the console \n",
    "    \"\"\"\n",
    "    print(\"---- %s -----\" %s)    \n",
    "    \n",
    "def see(s, v):\n",
    "    \"\"\"\n",
    "    Function to print a key and value to the console \n",
    "    \"\"\"\n",
    "    print(\"---- %s -----\" %s)\n",
    "    print(v)\n",
    "    \n",
    "def get_most_common(arr, ith):\n",
    "    \"\"\"\n",
    "    Function to return the ith commen value in a list and return empty string if the doesn't exist \n",
    "    \"\"\"\n",
    "    if len(arr) == 0:\n",
    "        return ''\n",
    "    if len(set(arr)) < ith:\n",
    "        return ''\n",
    "    else:\n",
    "        try:\n",
    "            cnt = Counter(arr)\n",
    "            item = cnt.most_common(ith)[ith-1][0]\n",
    "            return item;\n",
    "        except ValueError:\n",
    "            return '';\n",
    "\n",
    "def is_int(num):\n",
    "    \"\"\"\n",
    "    Function to check if a value is an integer or not \n",
    "    \"\"\"\n",
    "    try:\n",
    "        if num is None:\n",
    "            return False;\n",
    "        int(num)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def formatFinalDF(row):\n",
    "    \"\"\"\n",
    "    Function to format the final row output \n",
    "    \"\"\"\n",
    "    return [row[0],row[1],row[2],row[3],row[4],get_most_common(row[5],1),\n",
    "            get_most_common(row[5],2),row[6],'MOBILE', 'N/A','N/A']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Register the helper functions to be a user defined funciton in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_most_common_df = func.UserDefinedFunction(func=get_most_common, returnType=StringType())\n",
    "is_int_col = func.UserDefinedFunction(func=is_int, returnType=BooleanType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* define the path of the input file \n",
    "* define the dir path of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputFilePath = \"Data/MobileProtocol.20170327T221500.27784.udr\"\n",
    "ouputDir = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* start the spark session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"challenge1\").master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read the input file which represent in the comma seperated values file format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mobile_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(inputFilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here in this step i will exclude the bad rows which contains invalid subscriber_id which i assumed it will unique number for the subscriber and store it into **mobile_df_excluded** dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mobile_df_excluded = mobile_df.filter(is_int_col(mobile_df['SubscriberId']) == False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then i will filter the dataframe to contains only the valid subscriber_ids \n",
    "* The i removed the duplicates to avoid the duplication problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subscriber_mobile_df = mobile_df.filter(is_int_col(mobile_df['SubscriberId']))\n",
    "unique_subscriber_mobile_df = subscriber_mobile_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this step asked to exclude the duplicate rows into a seperate file and to do that i tried to show the power of dataframe apis which we can easy convert the dataframe into a rdd and the vice versa.\n",
    "\n",
    "* The way i was thinking is to form a paired rdd where the key is the entire row and the value is 1 then reduce this rdd by value to count the duplicates row then filters the row which count is greater than 1 and save them into a rdd to be written later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "duplicated_subscriber_mobile_rdd = subscriber_mobile_df.rdd.map(lambda line: (line,1)).reduceByKey(lambda v1,v2: v1+v2)\n",
    "duplicated_subscriber_mobile_rdd = duplicated_subscriber_mobile_rdd.filter(lambda line: line[1] != 1)\n",
    "duplicated_subscriber_mobile_rdd = duplicated_subscriber_mobile_rdd.map(lambda line: line[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this step i find the dataframe apis more easy to use and for better performance to summarize the subscriber data and aggregrate the following cols :\n",
    "    * TotalBytes : The total bytes used by the subscriber\n",
    "    * Total_Count: The total count of transaction for the subscriber \n",
    "    * Insertion_Date : The current date\n",
    "    * MostUsedServices: A list the contains the subscriber id services ** i did an assumption here that the services list is not tooo big to not be collect but if that is not a valid assumption i will also attached another way to calculate this value**\n",
    "    * Total_Time_In_Mins: The total time in mintues for the subscriber transactions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mobile_subscriber_summary_def = unique_subscriber_mobile_df.groupBy(\"SubscriberId\").agg(\n",
    "func.sum(\"TotalBytes\").alias(\"Total_Bytes\"),\n",
    "func.count(\"TotalBytes\").alias(\"Total_Count\"),\n",
    "func.current_date().alias(\"Insertion_Date\"),\n",
    "func.first('IMEI').alias('IMEI'),\n",
    "func.collect_list(\"ServiceName\").alias('MostUsedServices'),\n",
    "((func.unix_timestamp(func.max(\"EndTime\"))-func.unix_timestamp(func.min(\"StartTime\")))/60).alias('Total_Time_In_Mins'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* format the final rdd by adding the hardcoded values and save it to mobile_subscriber_summary_df to be written later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mobile_subscriber_summary_df = mobile_subscriber_summary_def.rdd.map(formatFinalDF).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write the subscriber summary data to the output file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mobile_subscriber_summary_df.write.csv(ouputDir+\"CH1_Job_out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write the exclued invalid subsriber rows  to the output file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mobile_df_excluded.rdd.saveAsTextFile(ouputDir+\"dpiRejection_out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write the duplicates subscriber transaction rows to the output file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "duplicated_subscriber_mobile_rdd.saveAsTextFile(ouputDir+\"dpiDublicate_out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes & Enchancements \n",
    "--------------------------------------------\n",
    "* As i mentioned i did an assumption that the subscriber_id services are not too big to be collected as list but anthoer way come to my mind is to: \n",
    "    * make a new paired rdd where the key is the (subscriber_id, service_name) and the value is 1 then reduce this rdd by key to count for each key how much it was commen then sort the this rdd descending and get the first item and so on \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subscriber_services = unique_subscriber_mobile_df.rdd.map(lambda line: ((line[7], line[19]), 1)).reduceByKey(lambda v1,v2: v1+v2)\n",
    "subscriber_services = subscriber_services.map(lambda line:(line[1],line[0])).sortByKey(False)\n",
    "subscriber_services = subscriber_services.map(lambda line: ((line[1][0], (line[1][1], line[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this task i used pyspark dataframe due to it's better performance also i tried to swith between dataframe and rdd and vice versa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One final item to mention that i will have alot of items to optimize already :( like first point i mentioned and utilize the dataframe perforamce i will work on them seperately and still there is a room of enhancement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
