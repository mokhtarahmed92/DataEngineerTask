{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge(1):\n",
    "----------------------------------\n",
    "## Subscriber data analysis using pyspark\n",
    "\n",
    "* The target of this task is to analyze subscriber data from telecommunication source system named DPI.\n",
    "\n",
    "## Input:\n",
    "-------------------------------------\n",
    "* Highly compressed file as tar + gzip and the extension .tgz The data schema provided as below  \n",
    "\n",
    "<img src=\"img/ch1_1.jpg\">\n",
    "\n",
    "## Ouput: \n",
    "------------------------------------\n",
    "1. New **CSV** file with the following columns :\n",
    "    * Account_Number : which map to the subscriber id in the input file \n",
    "    * MostUsedServiceName_1 : the 1st most used service by the subscriber \n",
    "    * MostUsedServiceName_2 : the 2nd most used service by the subscriber \n",
    "    * Event_Type : Hardcoded to 'Mobile'\n",
    "    * IMEI : Map to IMEI in the input file\n",
    "    * MAC_Address : Hardcoded to N/A\n",
    "    * Access_Point : Hardcoded to N/A\n",
    "    * Total_Bytes : Sum(TotalBytes) for the subscriberID \n",
    "    * Total_Count : Count of transactions\n",
    "    * Total_Time : Difference between (min(StartTime), max(EndTime)) for the subscriberID   \n",
    "    * Insertion_Date : Current_Date (yyyy-mm-dd) \n",
    "\n",
    "2. **dpiDuplicate** file which contains the duplicate transaction in the input file \n",
    "\n",
    "3. **dpiRejection** file which contains the rows of a bad subscriber_id **Which i assumed it will be an number**\n",
    "\n",
    "## Task Description:\n",
    "--------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* load the required spark apis and utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import * \n",
    "import pyspark.sql.functions as func\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define utilities and helper functions \n",
    "    * see(s): function to print a value \n",
    "    * see(s,v): function to pring a key and value \n",
    "    * get_most_common(arr, ith): function to return the ith commen used item in a list \n",
    "    * is_int(num): check if a value is an integer or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def see(s):\n",
    "    \"\"\"\n",
    "    Function to print a value to the console \n",
    "    \"\"\"\n",
    "    print(\"---- %s -----\" %s)    \n",
    "    \n",
    "def see(s, v):\n",
    "    \"\"\"\n",
    "    Function to print a key and value to the console \n",
    "    \"\"\"\n",
    "    print(\"---- %s -----\" %s)\n",
    "    print(v)\n",
    "    \n",
    "def is_int(num):\n",
    "    \"\"\"\n",
    "    Function to check if a value is an integer or not \n",
    "    \"\"\"\n",
    "    try:\n",
    "        if num is None:\n",
    "            return False;\n",
    "        int(num)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def extract_result(services):\n",
    "    \"\"\"\n",
    "    Function to extract the 1st and  the 2nd most used services\n",
    "    \"\"\"\n",
    "    services = list(services)\n",
    "    if len(services) == 0:\n",
    "        return ['','']\n",
    "    elif len(services) == 1:\n",
    "        return [services[0],'']\n",
    "    else:\n",
    "        return [services[0],services[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Register the helper functions to be a user defined funciton in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_int_col = func.UserDefinedFunction(func=is_int, returnType=BooleanType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* define the path of the input file \n",
    "* define the dir path of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputFilePath = \"Data/MobileProtocol.20170327T221500.27784.udr\"\n",
    "ouputDir = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* start the spark session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"challenge1\").master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read the input file which represent in the comma seperated values file format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mobile_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(inputFilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here in this step i will exclude the bad rows which contains invalid subscriber_id which i assumed it will unique number for the subscriber and store it into **mobile_df_excluded** dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mobile_df_excluded = mobile_df.filter(is_int_col(mobile_df['SubscriberId']) == False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then i will filter the dataframe to contains only the valid subscriber_ids \n",
    "* The i removed the duplicates to avoid the duplication problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subscriber_mobile_df = mobile_df.filter(is_int_col(mobile_df['SubscriberId']))\n",
    "unique_subscriber_mobile_df = subscriber_mobile_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this step asked to exclude the duplicate rows into a seperate file and to do that i tried to show the power of dataframe apis which we can easy convert the dataframe into a rdd and the vice versa.\n",
    "\n",
    "* The way i was thinking is to form a paired rdd where the key is the entire row and the value is 1 then reduce this rdd by value to count the duplicates row then filters the row which count is greater than 1 and save them into a rdd to be written later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "duplicated_subscriber_mobile_rdd = subscriber_mobile_df.rdd.map(lambda line: (line,1)).reduceByKey(lambda v1,v2: v1+v2)\n",
    "duplicated_subscriber_mobile_rdd = duplicated_subscriber_mobile_rdd.filter(lambda line: line[1] != 1)\n",
    "duplicated_subscriber_mobile_rdd = duplicated_subscriber_mobile_rdd.map(lambda line: line[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this step i find the dataframe apis more easy to use and for better performance to summarize the subscriber data and aggregrate the following cols :\n",
    "    * TotalBytes : The total bytes used by the subscriber\n",
    "    * Total_Count: The total count of transaction for the subscriber \n",
    "    * Insertion_Date : The current date\n",
    "    * Total_Time_In_Mins: The total time in mintues for the subscriber transactions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mobile_subscriber_summary_df = unique_subscriber_mobile_df.groupBy(\"SubscriberId\").agg(\n",
    "func.sum(\"TotalBytes\").alias(\"Total_Bytes\"),\n",
    "func.count(\"TotalBytes\").alias(\"Total_Count\"),\n",
    "func.current_date().alias(\"Insertion_Date\"),\n",
    "func.first('IMEI').alias('IMEI'),\n",
    "((func.unix_timestamp(func.max(\"EndTime\"))-func.unix_timestamp(func.min(\"StartTime\")))/60).alias('Total_Time_In_Mins'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now i will constrcut the subscriber Most frequent used services dataframe for more details please read teh Notes section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subscriber_services = unique_subscriber_mobile_df.rdd.map(lambda line: ((line[7], line[19]), 1)).reduceByKey(lambda v1,v2: v1+v2)\n",
    "subscriber_services = subscriber_services.map(lambda line:(line[1],line[0])).sortByKey(False)\n",
    "subscriber_services = subscriber_services.map(lambda line: line[1])\n",
    "subscriber_services = subscriber_services.groupByKey().map(lambda line: (line[0], extract_result(line[1])[0],extract_result(line[1])[1]))\n",
    "subscriber_services = subscriber_services.toDF([\"SubscriberId\", \"MostUsedServiceName_1\", \"MostUsedServiceName_2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then i will join subscriber_id services dataframe with the summary dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_subscriber_summary_df = mobile_subscriber_summary_df.join(subscriber_services,['SubscriberId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this step i will add the hardcoded columns like **(\"Event_Type\", \"MAC_Address\", \"Access_Point\")**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_subscriber_summary_df = mobile_subscriber_summary_df.withColumn(\"Event_Type\", func.lit(\"Mobile\"))\n",
    "mobile_subscriber_summary_df = mobile_subscriber_summary_df.withColumn(\"MAC_Address\", func.lit(\"N/A\"))\n",
    "mobile_subscriber_summary_df = mobile_subscriber_summary_df.withColumn(\"Access_Point \", func.lit(\"N/A\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write the subscriber summary data to the output file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_subscriber_summary_df.write.csv(ouputDir+\"CH1_Job1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write the exclued invalid subsriber rows  to the output file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mobile_df_excluded.rdd.saveAsTextFile(ouputDir+\"dpiRejection1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write the duplicates subscriber transaction rows to the output file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_subscriber_mobile_rdd.saveAsTextFile(ouputDir+\"dpiDublicate1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes & Enchancements \n",
    "--------------------------------------------\n",
    "* As i mentioned i did an assumption that the subscriber_id services are not too big to be collected as list but anthoer way come to my mind is to: \n",
    "    * make a new paired rdd where the key is the (subscriber_id, service_name) and the value is 1 then reduce this rdd by key to count for each key how much it was commen then sort the this rdd descending.\n",
    "    * After grouping the resulted rdd by key we will get a list a list of unique services names sorted in desc order \n",
    "    * Map the grouped rdd and select the first and the second elements if avaliable and if not return empty strings instead\n",
    "    * Then we can join the resulted paired rdd **subscriber_services** with **mobile_subscriber_summary_def** on the subscriber_id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscriber_services = unique_subscriber_mobile_df.rdd.map(lambda line: ((line[7], line[19]), 1)).reduceByKey(lambda v1,v2: v1+v2)\n",
    "subscriber_services = subscriber_services.map(lambda line:(line[1],line[0])).sortByKey(False)\n",
    "subscriber_services = subscriber_services.map(lambda line: line[1])\n",
    "subscriber_services = subscriber_services.groupByKey().map(lambda line: (line[0], extract_result(line[1])[0],extract_result(line[1])[1]))\n",
    "subscriber_services = subscriber_services.toDF([\"SubscriberId\", \"MostUsedServiceName_1\", \"MostUsedServiceName_2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this task i used pyspark dataframe due to it's better performance also i tried to swith between dataframe and rdd and vice versa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One final item to mention that i will have alot of items to optimize already :( like first point i mentioned and utilize the dataframe perforamce i will work on them seperately and still there is a room of enhancement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
